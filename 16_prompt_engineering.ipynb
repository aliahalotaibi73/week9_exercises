{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a910d17de618481ab20a4d8de4c7e85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d905bf9a9964cd0b71de96ca17d840c",
              "IPY_MODEL_ab079656915044dfbc188483864ea9f8",
              "IPY_MODEL_cd9eced9a95649848955a01f161f9638"
            ],
            "layout": "IPY_MODEL_c64c1c6426604820844af95300e54050"
          }
        },
        "6d905bf9a9964cd0b71de96ca17d840c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a6772125f0c44b3888a611fb05212d9",
            "placeholder": "​",
            "style": "IPY_MODEL_38cd25da8f624a14b94fb80ee0da9f58",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ab079656915044dfbc188483864ea9f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ee933e7afaf4d30ad0436cb719c555b",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91ac140660ab449186fd6e03d701ec75",
            "value": 8
          }
        },
        "cd9eced9a95649848955a01f161f9638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_656f9940bb9e47db88c8abfba3530b9c",
            "placeholder": "​",
            "style": "IPY_MODEL_0d2ee7884ad0484580befaaa5995078d",
            "value": " 8/8 [01:11&lt;00:00,  7.61s/it]"
          }
        },
        "c64c1c6426604820844af95300e54050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a6772125f0c44b3888a611fb05212d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38cd25da8f624a14b94fb80ee0da9f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ee933e7afaf4d30ad0436cb719c555b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91ac140660ab449186fd6e03d701ec75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "656f9940bb9e47db88c8abfba3530b9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d2ee7884ad0484580befaaa5995078d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Engineering\n",
        "\n",
        "We will exercise prompt engineering using a `text-generation` model via the [ChatHuggingFace](https://python.langchain.com/docs/integrations/chat/huggingface/) API."
      ],
      "metadata": {
        "id": "u7prTY6pSS28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "aO7HEdIVSlAl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if not os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Enter your token: \")"
      ],
      "metadata": {
        "id": "r-cn-HXgSP0j"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantization\n",
        "\n",
        "Quantization techniques **reduce memory and computational costs** by representing weights and activations with lower-precision data types like 8-bit integers (int8).\n",
        "\n",
        "This enables loading larger models you normally wouldn’t be able to fit into memory, and speeding up inference. Transformers supports the AWQ and GPTQ quantization algorithms and it supports 8-bit and 4-bit quantization with `bitsandbytes`."
      ],
      "metadata": {
        "id": "EstGmAHVXUhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ],
      "metadata": {
        "id": "CVHMce1-SnjC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2021/07/qat-training-precision.png\" width=\"600\">\n",
        "\n",
        "Image Source: [nvidia.com/blog](https://developer.nvidia.com/blog/achieving-fp32-accuracy-for-int8-inference-using-quantization-aware-training-with-tensorrt/)"
      ],
      "metadata": {
        "id": "KnByTHXeYjkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    pipeline_kwargs=dict(\n",
        "        max_new_tokens=512, # (clip if the model doesn't stop itself)\n",
        "        do_sample=True,  # Enable sampling (will depend on temperature)\n",
        "        temperature=0.35, # Temperature of 0 is equivalent of no sampling (Greedy)\n",
        "        repetition_penalty=1.03,\n",
        "        return_full_text=False, # Determines whether to return the entire generated text or only the last generated token\n",
        "    ),\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm)"
      ],
      "metadata": {
        "id": "hM8otggtSpxF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "a910d17de618481ab20a4d8de4c7e85f",
            "6d905bf9a9964cd0b71de96ca17d840c",
            "ab079656915044dfbc188483864ea9f8",
            "cd9eced9a95649848955a01f161f9638",
            "c64c1c6426604820844af95300e54050",
            "6a6772125f0c44b3888a611fb05212d9",
            "38cd25da8f624a14b94fb80ee0da9f58",
            "3ee933e7afaf4d30ad0436cb719c555b",
            "91ac140660ab449186fd6e03d701ec75",
            "656f9940bb9e47db88c8abfba3530b9c",
            "0d2ee7884ad0484580befaaa5995078d"
          ]
        },
        "outputId": "937209ae-1157-4111-a8d6-8be405644b58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a910d17de618481ab20a4d8de4c7e85f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_huggingface.llms.huggingface_pipeline:Setting the `device` argument to None from -1 to avoid the error caused by attempting to move the model that was already loaded on the GPU using the Accelerate module to the same or another device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `temperature`\n",
        "- In short, the lower the temperature, the more deterministic the results in the sense that the highest probable next token is always picked.\n",
        "- Increasing temperature could lead to more randomness, which encourages more diverse or creative outputs.\n",
        "- In terms of application, you might want to use a lower temperature value for tasks like fact-based QA to encourage more factual and concise responses.\n",
        "- For poem generation or other creative tasks, it might be beneficial to increase the temperature value.\n",
        "\n"
      ],
      "metadata": {
        "id": "kRVKSwHaW1UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*J37qonVPJvKZpzv2\" width=\"600\">\n",
        "\n",
        "Image Source: [How to sample from language models | by Ben Mann | Towards Data Science]"
      ],
      "metadata": {
        "id": "Y-IhHu1OYOz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `max_new_tokens`\n",
        "\n",
        "Specifying a max length helps you prevent long or irrelevant responses and **control costs**.\n"
      ],
      "metadata": {
        "id": "RSG0eluLXilf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nkkoXPHCO8Bq"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're a helpful assistant\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# user_input = \"What happens when an unstoppable force meets an immovable object?\"\n",
        "user_input = input(\"Tell the AI something: \")"
      ],
      "metadata": {
        "id": "F5uT7bvTZPyC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8431c63d-195e-4449-c322-f08b21ab30c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tell the AI something: How do you feel today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(HumanMessage(content=user_input))"
      ],
      "metadata": {
        "id": "0OEkSp4uZLKX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "cIdqyezMZrrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f59b9a4e-c03d-499f-f863-c6b0ad81acf3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"You're a helpful assistant\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='How do you feel today?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg = chat_model.invoke(messages)"
      ],
      "metadata": {
        "id": "OxFcNQlXZmt1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "R3QlvJbQTAhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da7a71a8-36b5-4bb3-bfae-b88349e28409"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I do not have feelings or emotions as humans do. My primary function is to provide helpful information and assist with tasks based on the input provided by users. I am programmed to respond in a polite and helpful manner, but I do not have personal feelings or opinions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your Turn\n",
        "\n",
        "**Exercise 1**: Experiment with different prompts to achieve different tasks:\n",
        "\n",
        "1. Summarization\n",
        "2. Translation\n",
        "3. Classification\n",
        "4. Get creative and make up your own task\n",
        "\n",
        "Instructions can be as long as you'd like. Be clear about:\n",
        "- The task you want to be achieved by the LLM\n",
        "- Any constraints you want the LLM to take care of\n",
        "- The format, style, tone, and phrasing of the response you want from the LLM\n",
        "\n",
        "Hint: you need to change the `SystemMessage(content=\">>INSTRUCTIONS<<\")` to include your instructions.\n",
        "\n",
        "Resources:\n",
        "- [Prompt Engineering Best Practices](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering#best-practices)\n",
        "- [Prompt Engineering Techniques](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions)"
      ],
      "metadata": {
        "id": "WljqeJMdTL57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE DOWN HERE\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"\"\"This is T5 boootcamp, and it will guide you to learn Data science and AI.\n",
        "     From this bootcamp you will learn how to clean dataset, learn the pattern, make decision and feature engineering. \"\"\")\n",
        "]"
      ],
      "metadata": {
        "id": "fwvVliTOTfB6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Exercise 2**: use a different LLM and compare the results by eye.\n",
        "\n",
        "Hint: you need to change `model_id` to be some `text-generation` model from [HuggingFace Hub](https://huggingface.co/models?pipeline_tag=text-generation)."
      ],
      "metadata": {
        "id": "gTxz7nE1VFg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE DOWN HERE\n",
        "user_input = input(\"Tell the AI something: \")"
      ],
      "metadata": {
        "id": "6VcFWzTGVGI3",
        "outputId": "5745a73d-7dde-4078-acdb-d6a75d65790c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tell the AI something: does this paragraph written by a GPT?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(HumanMessage(content=user_input))"
      ],
      "metadata": {
        "id": "wn8-BsSvAShC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "0f-eNEpBAS5v",
        "outputId": "5262a7ac-7e7a-4e01-de89-37e01d19e292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='This is T5 boootcamp, and it will guide you to learn Data science and AI.\\n     From this bootcamp you will learn how to clean dataset, learn the pattern, make decision and feature engineering. ', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Can you summerize this paragraph?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='can you Translation this to arabic?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='is this a spam message?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='does this text write by a GPT?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='does this paragraph written by a GPT?', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg = chat_model.invoke(messages)"
      ],
      "metadata": {
        "id": "-tuG5nO0AVhE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "3Z56fF1rAYAL",
        "outputId": "99f5390f-69a0-48f7-e438-04414ca35d01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, this paragraph is likely written by a GPT (Generative Pre-trained Transformer), which is a type of artificial intelligence language model developed by OpenAI. GPTs can generate human-like text on a variety of topics, but they are not conscious or sentient beings, and they do not have the ability to understand or interpret the meaning of the text they generate.\n"
          ]
        }
      ]
    }
  ]
}